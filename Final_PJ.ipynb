{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "import numpy as np\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings_raw_data = sc.textFile('550_finalPJ/ratings.csv')\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]\n",
    "\n",
    "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
    ".map(lambda line : line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2]))\\\n",
    ".map(lambda x: (int(x[0]), int(x[1]), float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_movies_raw_data = sc.textFile('550_finalPJ/movies.csv')\n",
    "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]\n",
    "\n",
    "small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
    ".map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1]))\\\n",
    ".map(lambda x: (int(x[0]), x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ______Checking data set: \n",
    "#### small_ratings_data = (userID, movieID, rating)\n",
    "#### small_movies_data = (movieID, movieName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1, 4.0), (1, 3, 4.0), (1, 6, 4.0), (1, 47, 5.0), (1, 50, 5.0), (1, 70, 3.0), (1, 101, 5.0), (1, 110, 4.0), (1, 151, 5.0), (1, 157, 5.0)]"
     ]
    }
   ],
   "source": [
    "small_ratings_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Toy Story (1995)'), (2, 'Jumanji (1995)'), (3, 'Grumpier Old Men (1995)'), (4, 'Waiting to Exhale (1995)'), (5, 'Father of the Bride Part II (1995)'), (6, 'Heat (1995)'), (7, 'Sabrina (1995)'), (8, 'Tom and Huck (1995)'), (9, 'Sudden Death (1995)'), (10, 'GoldenEye (1995)')]"
     ]
    }
   ],
   "source": [
    "small_movies_data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training set and test set: 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, test_RDD = small_ratings_data.randomSplit([8, 2], seed=0)\n",
    "test_user_unwatch = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___Checking data set:\n",
    "#### Numbers of training data and test data\n",
    "#### test samples for prediction: (userID, unWatchedID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of training dataset is 80720\n",
      "The total number of test dataset is 20116\n",
      "Rate of training and test: 4.012726188108968"
     ]
    }
   ],
   "source": [
    "Total_train = training_RDD.count()\n",
    "Total_test = test_RDD.count()\n",
    "print(\"The total number of training dataset is\", Total_train)\n",
    "print(\"The total number of test dataset is\", Total_test)\n",
    "print(\"Rate of training and test:\", Total_train/Total_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 70), (1, 101), (1, 110), (1, 151), (1, 216), (1, 316), (1, 333), (1, 356), (1, 367), (1, 500)]"
     ]
    }
   ],
   "source": [
    "test_user_unwatch.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the rating of movie: subtract mean rating $m_{i}$ from each movie i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_means = training_RDD.map(lambda x: (x[1], (x[2], 1))).reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
    ".map(lambda x: (x[0], x[1][0]/x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ratings = training_RDD.map(lambda x: (x[1], (x[0], x[2]))).join(movie_means)\\\n",
    ".map(lambda x: (x[1][0][0], x[0], x[1][0][1] - x[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___Checking dataset:\n",
    "#### movie_means: (movie, mean of ratings)\n",
    "#### normalized_ratings: (userID, movieID, norm_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 3.98125), (50, 4.2398843930635834), (260, 4.21875), (296, 4.242738589211618), (362, 3.3541666666666665), (480, 3.751322751322751), (552, 3.223404255319149), (590, 3.7934782608695654), (592, 3.462025316455696), (596, 3.5104166666666665)]"
     ]
    }
   ],
   "source": [
    "movie_means.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(328, 5060, 0.13235294117647056), (368, 5060, 0.13235294117647056), (372, 5060, -1.8676470588235294), (385, 5060, -0.8676470588235294), (387, 5060, 0.13235294117647056), (409, 5060, 0.13235294117647056), (414, 5060, 0.13235294117647056), (465, 5060, 1.1323529411764706), (469, 5060, 0.13235294117647056), (474, 5060, -0.8676470588235294)]"
     ]
    }
   ],
   "source": [
    "normalized_ratings.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting cosine similarity with Pearson correlation coefficient\n",
    "### $S_{xy} = $ items rated by both user x and user y\n",
    "### $m_{1}$ and $m_{2}$ are normalized movie ratings\n",
    "\n",
    "<font size = \"5\">\n",
    "\n",
    "$$\n",
    "sim =  \\frac{\\sum_{s \\in S_{xy} } m_{1} \\times m_{2}}{ \\sqrt{\\sum_{s \\in S_{xy} } m_{1}^{2}} \\sqrt{\\sum_{s \\in S_{xy} } m_{2}^{2}} }\n",
    "$$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_perm(line):\n",
    "    perm = list(permutations(line, 2))\n",
    "    return perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = normalized_ratings.map(lambda x: (x[0], (x[1], x[2]))).groupByKey()\\ # (user, list((movie, ratings)))\n",
    ".flatMap(lambda x: item_perm(list(x[1])))\\ # ((m1, r1), (m2, r2))\n",
    ".map(lambda x: ((x[0][0], x[1][0]),(x[0][1]*x[1][1], x[0][1]**2, x[1][1]**2)))\\ # ((m1, m2),(r1*r2, r1^2, r2^2))\n",
    ".reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2]))\\ #((m1, m2), (sum(r1*r2), sum(r1^2), sum(r2^2)))\n",
    ".map(lambda x: (x[0], x[1][0]/ np.sqrt(x[1][1]) / np.sqrt(x[1][2]))).cache() # ((m1,m2), sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ____Checking dataset:\n",
    "#### cosine_sim: (movie_1, movie_2, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1732, 1722), -0.15349541830097294), ((2288, 230), 1.0), ((2112, 3370), 1.0), ((3768, 858), 0.0), ((350, 736), 0.3633622554647428), ((266, 1616), 0.4172599427706273), ((2402, 3208), 1.0), ((3430, 1676), 1.0), ((555, 2347), -1.0), ((2803, 431), -0.5174798063860673)]"
     ]
    }
   ],
   "source": [
    "cosine_sim.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_unit = movie_pairs.map(lambda x: (x[0], x[1][0][0]*x[1][0][1]/x[1][1][0]/x[1][1][1]))\n",
    "cosine_dist = cosine_unit.reduceByKey(lambda x, y : x + y).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((5060, 934), 0.0009350619880827025), ((1676, 3006), 0.008327630580662328), ((1610, 208), 0.002095698252686468), ((2174, 2716), 0.006398772712746157), ((3210, 748), -0.0034375528847518993), ((3006, 1500), 0.004453708718907958), ((2302, 1204), 0.0032704276843571805), ((1690, 648), 0.011557282922293674), ((1754, 3032), 0.004933541706660469), ((3846, 736), 0.0019309767679504342)]"
     ]
    }
   ],
   "source": [
    "cosine_dist.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get pairs in one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_join(line):\n",
    "    perm = list(permutations(line, 2))\n",
    "    return perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_pairs = normalized_ratings.map(lambda line: (line[0], (line[1], line[2], line[3]))).groupByKey()\\\n",
    ".map(lambda line: list(line[1])).flatMap(lambda line: item_join(line))\\\n",
    ".map(lambda line: ((line[0][0], line[1][0]),((line[0][1], line[1][1]), (line[0][2], line[1][2])))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1, 733), ((-0.9057142857142857, 1.33), (52.822817039608935, 37.73592452822641))), ((1, 1517), ((-0.9057142857142857, 0.44666666666666677), (52.822817039608935, 31.682013824881775))), ((1, 141), ((-0.9057142857142857, -0.4675324675324677), (52.822817039608935, 31.368774282716245))), ((1, 1405), ((-0.9057142857142857, 0.94), (52.822817039608935, 16.439282222773596))), ((1, 673), ((-0.9057142857142857, -1.9324324324324325), (52.822817039608935, 19.04599695474091))), ((1, 661), ((-0.9057142857142857, -0.37804878048780477), (52.822817039608935, 22.621892051727237))), ((1, 605), ((-0.9057142857142857, -2.0), (52.822817039608935, 9.40744386111339))), ((1, 1429), ((-0.9057142857142857, 1.2999999999999998), (52.822817039608935, 14.637281168304447))), ((1, 805), ((-0.9057142857142857, 0.3793103448275863), (52.822817039608935, 19.987496091306685))), ((1, 737), ((-0.9057142857142857, 2.3055555555555554), (52.822817039608935, 12.698425099200294)))]"
     ]
    }
   ],
   "source": [
    "movie_pairs.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get baseline estimation for user-item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = training_RDD.map(lambda x: float(x[2]))\n",
    "total = ratings.sum()\n",
    "total_num = ratings.count()\n",
    "global_mean = total/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.502682111000991"
     ]
    }
   ],
   "source": [
    "global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_average = movie_mean_form.map(lambda line: (line[0], line[1][0]))\n",
    "user_average = training_RDD.map(lambda x: (int(x[0]), (float(x[2]), 1)))\\\n",
    ".reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).map(lambda x: (x[0], x[1][0]/x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ave_dict = {x[0]: x[1] for x in movie_average.collect()}\n",
    "user_ave_dict = {x[0]: x[1] for x in user_average.collect()}\n",
    "\n",
    "def baseLine(user, movie):\n",
    "    if movie not in movie_ave_dict:\n",
    "        return user_ave_dict[user]\n",
    "    return user_ave_dict[user] + movie_ave_dict[movie] - global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_RDD.map(lambda line: ((int(line[0]), int(line[1])), float(line[2])))\n",
    "movie_sim = cosine_dist.map(lambda x: (x[0][0], (x[0][1], x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_for_predict_RDD.map(lambda x: (int(x[1]). int(x[0])))\n",
    "training_data = training_RDD.map(lambda line: ((int(line[0]), int(line[1])), float(line[2])))\n",
    "movie_sim = cosine_dist.map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
    "tmp = movie_sim.join(test_data).map(lambda x: ((x[1][1], x[1][0][0]), (x[0], x[1][0][1])))\n",
    "predict_unit = training_data.join(tmp)\\\n",
    ".map(lambda x: ((x[0][0], x[1][1][0]), (x[1][1][1] * (x[1][0] - baseLine(x[0][0], x[0][1])), x[1][1][1])))\n",
    "prediction = predict_unit.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
    ".map(lambda x: (x[0], baseLine(x[0][0], x[0][1]) + x[1][0]/x[1][1]) if x[1][1]!= 0 else (x[0], baseLine(x[0][0], x[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value = test_RDD.map(lambda x: ((int(x[0]), int(x[1])), float(x[2])))\n",
    "comparison = prediction.join(test_value).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_1 = comparison.map(lambda x: x[0])\n",
    "part_2 = test_data.map(lambda x: (x[1], x[0])).subtract(part_1)\\\n",
    ".map(lambda x: (x, baseLine(x[0], x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_comparison = part_2.join(test_value)\n",
    "Total_compar = comparison.union(part2_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 24.0 failed 4 times, most recent failure: Lost task 4.3 in stage 24.0 (TID 39, data2.cs.rutgers.edu, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "AttributeError: 'int' object has no attribute 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "AttributeError: 'int' object has no attribute 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/pyspark.zip/pyspark/rdd.py\", line 1343, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/pyspark.zip/pyspark/context.py\", line 1002, in runJob\n",
      "    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 24.0 failed 4 times, most recent failure: Lost task 4.3 in stage 24.0 (TID 39, data2.cs.rutgers.edu, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "AttributeError: 'int' object has no attribute 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "AttributeError: 'int' object has no attribute 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Total_compar.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 35.0 failed 4 times, most recent failure: Lost task 4.3 in stage 35.0 (TID 49, data2.cs.rutgers.edu, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "AttributeError: 'int' object has no attribute 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "AttributeError: 'int' object has no attribute 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/pyspark.zip/pyspark/rdd.py\", line 1032, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/pyspark.zip/pyspark/rdd.py\", line 906, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/pyspark.zip/pyspark/rdd.py\", line 809, in collect\n",
      "    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/hadoop/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000001/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 35.0 failed 4 times, most recent failure: Lost task 4.3 in stage 35.0 (TID 49, data2.cs.rutgers.edu, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "AttributeError: 'int' object has no attribute 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/hadoop/yarn/local/usercache/wl497/appcache/application_1554300167658_0048/container_e69_1554300167658_0048_01_000003/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<stdin>\", line 1, in <lambda>\n",
      "AttributeError: 'int' object has no attribute 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAE_tmp = Total_compar.map(lambda x: abs(x[1][0] - x[1][1])).sum()\n",
    "RMSE_tmp = Total_compar.map(lambda x: (x[1][0] - x[1][1])**2).sum()\n",
    "num = Total_compar.count()\n",
    "MAE = MAE_tmp/num\n",
    "RMSE = np.sqrt(RMSE_tmp/num)\n",
    "print(\"The MAE for the CF prediction is: \", MAE)\n",
    "print(\"The RMSE for the CF prediction is: \", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUnwatched(user, movies):\n",
    "    unwat_mov = list(movie_set - movies)\n",
    "    unwat_user = [(x, user) for x in unwat_mov]\n",
    "    return unwat_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_watched = training_RDD.map(lambda x: (int(x[0]), int(x[1]))).groupByKey()\\\n",
    ".map(lambda x: (x[0], set(x[1])))\n",
    "movie_set = set(small_movies_data.map(lambda x: int(x[0])).collect())\n",
    "unWatched_user = user_watched.flatMap(lambda x: getUnwatched(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 328), (3, 328), (4, 328), (5, 328), (6, 328), (7, 328), (8, 328), (9, 328), (10, 328), (11, 328)]"
     ]
    }
   ],
   "source": [
    "unWatched_user.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_sim.join(unWatched_user): (unwatched,((watched, sim), user)) ->\n",
    "#sim_unWatched: ((user, watched),(unwatched, sim))\n",
    "sim_unWatched = movie_sim.join(unWatched_user).map(lambda x: ((x[1][1], x[1][0][0]), (x[0], x[1][0][1])))\n",
    "#rate_sim: ((user, watched),(rating, (unwatched, sim)))\n",
    "rate_sim = training_data.join(sim_unWatched)\n",
    "predict_unit_recom = rate_sim.map(lambda x: ((x[0][0], x[1][1][0]), (x[1][1][1] *(x[1][0] - baseLine(x[0][0], x[0][1])) , x[1][1][1])))\n",
    "prediction_recom1 = predict_unit_recom.reduceByKey(lambda x, y : (x[0] + y[0], x[1] + y[1]))\\\n",
    ".map(lambda x: (x[0], baseLine(x[0][0], x[0][1]) + x[1][0]/x[1][1]) if x[1][1]!=0 else (x[0], baseLine(x[0][0], x[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseTen(pairs):\n",
    "    pairs.sort(key = lambda pair: pair[1], reverse = True)\n",
    "    return pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = prediction_recom1.map(lambda x: (x[0][0], (x[0][1], x[1]))).groupByKey().map(lambda x: (x[0], list(x[1])))\\\n",
    ".map(lambda x: (x[0], chooseTen(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://data-services2.cs.rutgers.edu:8999/sessions/994 with error payload: \"Session '994' not found.\"\n"
     ]
    }
   ],
   "source": [
    "prediction_recom1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26, 5.2), (1, 4.7)]"
     ]
    }
   ],
   "source": [
    "test_pairs = [(1,4.7), (10, 3.2), (123, 2.3), (26, 5.2)]\n",
    "chooseTen(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_numerator = movie_pairs.map(lambda x: (x[0], x[1][0]*x[1][1])).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2cb1a58bebe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movie_numerator.take(10)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2165\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2167\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-123>\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"ENCOUNTERED AN INTERNAL ERROR: {}\\n\\tTraceback:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Do not log! as some messages may contain private client information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/sparkmagic/kernels/kernelmagics.py\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda3/lib/python3.6/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "movie_numerator.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get pairs for movies denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_module = training_RDD.map(lambda x: (int(x[1]), float(x[2])**2)).reduceByKey(lambda x, y: x+y)\\\n",
    ".map(lambda x: (x[0], np.sqrt(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_mode_cart = movie_module.cartesian(movie_module).map(lambda x: ((x[0][0], x[1][0]), x[0][1]*x[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_dist = movie_numerator.join(movie_mode_cart).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_dist.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom = cosine_dist.map(lambda x: (x[0], x[1][0]/x[1[1]])).map(lambda x: (x[1], x[0])).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
